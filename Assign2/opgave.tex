\documentclass{article}

\usepackage{upgreek}
\usepackage{amsmath}
\usepackage{minted}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathrsfs}
\usepackage[toc,page]{appendix}

%Section style
\usepackage{etoolbox} %for configuration of sloppy
\usepackage{xcolor}


\definecolor{secnum}{RGB}{102,102,102}

\makeatletter
    \def\@seccntformat#1{\llap{\color{secnum}\csname the#1\endcsname\hskip 16pt}}
\makeatother
%end section style

\begin{document}

\section{II.1}

\subsection{II.1.1}

We implemented the LDA ourselves. The code can be seen in
Part1/Opgavei11.m. 

On the training data we observe a 14.00 \% miss rate while on the test data
the rate is 21.05 \%.

\subsection{II.1.2}

After normalizing the data we observed the same error as on the
non-transformed data. 

This is unsurprising as normalization preserves the relation inbetween
the elements of the dataset. Graphically a plot of the data set would
look identical except for a change of the axises. Finally LDA uses the
covariance which fully describes the distribution of the data and thus
renders normalization redundant.

\subsection{II.1.3}

Muh Bayes.

\newpage
\subsection{II.2.1}

We used the linear model
\begin{equation}
    y(x, w) = w_0 + w_1 x_1 + ... + w_D x_D
\end{equation}
and calculated the RMS using the python library sklearn. this gave us the fittings seen on 
figure \ref{fig:II21}. It seams Selection 2 provides the best prediction, but giving the problem some though, we concluded it mearly copies
the previous years sunspot numbers. So even if it does have the lowest RMS, it is not guarantede to be the best model.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{Part2/II211.png}
        \caption{fitting with $RMS = 64.4180$}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{Part2/II212.png}
        \caption{fitting with $RMS = 30.1460$}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{Part2/II213.png}
        \caption{fitting with $RMS = 49.8736$}
    \end{subfigure}
    \caption{Fittings using linear regression and ML}
    \label{fig:II21}
\end{figure}


\subsection{II.2.2}

On figure \ref{fig:II22} the plots of our MAP estimations are shown in relation
to our ML solutions. In each picture, the blue line is the estimation of
our Maximum likelihood solution and the read our MAP estimations given 
different alpha values, plottet on the x-axis.\\
According to our calculations, all the Selections has the best alpha values
when $\alpha \rightarrow 0$, with the Selection 1 ML solution never crossing 
our MAP solution. In Selection 2 and 3, the MAP values solutions is worse then the
ML solutions at respectivly $alpha \sim 10^{23}$ and $alpha \sim 10^{30}$

% We used the following to calculate our maximum posterior weight vector.\\
% \begin{align*}
    % p(w|\alpha) &= \mathcal{N}(w|0, \alpha^{-1}\textbf{I})\\
    % m_{N} &= \beta S_{N} \Phi^{T} \textbf{t}\\
    % S_{N} &= (\alpha \textbf{I} \cdot \beta \Phi^{T} \Phi)^{-1}
% \end{align*}
% We also know $w_{MAP} = m_N$.\\

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Part2/II221.png}
        \caption{Selection 1}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Part2/II222.png}
        \caption{Selection 2}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Part2/II223.png}
        \caption{Selection 3}
    \end{subfigure}
    \caption{MLS vs MAP}
    \label{fig:II22}
\end{figure}

% 1:
% bestmN = [[-0.25975759], [ 0.99400395]]
% bestrms = 33.6875907352
% bestAlpha = 10

% 2:
% bestmN = [[ 0.93127615]]
% bestrms = 20.8018430001
% bestAlpha = 10

% 3:
% bestmN = [[-0.02833114], [ 0.18767638], [ 0.03828809], [-0.57547992], [ 1.35501008]]
% bestrms = 14.5218332936
% bestAlpha = 10

\subsection{II.2.3}

\end{document}
