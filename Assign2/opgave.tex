\documentclass{article}

\usepackage{upgreek}
\usepackage{amsmath}
\usepackage{minted}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathrsfs}
\usepackage[toc,page]{appendix}

%Section style
\usepackage{etoolbox} %for configuration of sloppy
\usepackage{xcolor}


\definecolor{secnum}{RGB}{102,102,102}

\makeatletter
    \def\@seccntformat#1{\llap{\color{secnum}\csname the#1\endcsname\hskip 16pt}}
\makeatother
%end section style

\begin{document}

\section{II.1}

\subsection{II.1.1}

We implemented the LDA ourselves. The code can be seen in
Part1/Opgavei11.m. 

On the training data we observe a 14.00 \% miss rate while on the test data
the rate is 21.05 \%.

\subsection{II.1.2}

After normalizing the data we observed the same error as on the
non-transformed data. 

This is unsurprising as normalization preserves the relation inbetween
the elements of the dataset. Graphically a plot of the data set would
look identical except for a change of the axises. Finally LDA uses the
covariance which fully describes the distribution of the data and thus
renders normalization redundant.

\subsection{II.1.3}

To calculate the risk of the probalistic classifier you have to
calculate the chance of the predicted result being incorrect. In this
case with two possible outputs, it is simply frequency of '0' times the
probability of '1' + frequency of '1' times the probability of '0'.\\

\begin{align*}
  &\mathcal{R}_s(h) = {h(0) \neq y_1} + {h(1) \neq y_1}\\
  &\Downarrow\\
  &\mathcal{R}_is(h) = 0.25 * 0.75 + 0.75 * 0.25 = 37.5 \%
\end{align*}

\subsection{II.2.1}

We used the linear model
\begin{equation}
    y(x, w) = w_0 + w_1 x_1 + ... + w_D x_D
\end{equation}
and calculated the RMS using the python library sklearn. this gave us the fittings seen on 
figure \ref{fig:II21}. It seams Selection 2 provides the best prediction, but giving the problem some though, we concluded it mearly copies
the previous years sunspot numbers. So even if it does have the lowest RMS, it is not guarantede to be the best model.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{Part2/II211.png}
        \caption{fitting with $RMS = 64.4180$}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{Part2/II212.png}
        \caption{fitting with $RMS = 30.1460$}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{Part2/II213.png}
        \caption{fitting with $RMS = 49.8736$}
    \end{subfigure}
    \caption{Fittings using linear regression and ML}
    \label{fig:II21}
\end{figure}


\subsection{II.2.2}

We used the following to calculate our maximum posterior weight vector.\\
\begin{align*}
    % p(w|\alpha) &= \mathcal{N}(w|0, \alpha^{-1}\textbf{I})\\
    m_{N} &= \beta S_{N} \Phi^{T} \textbf{t}\\
    S_{N} &= (\alpha \textbf{I} \cdot \beta \Phi^{T} \Phi)^{-1}
\end{align*}
We also know $w_{MAP} = m_N$.\\

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{Part2/II221.png}
        \caption{Selection 1}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{Part2/II222.png}
        \caption{Selection 2}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{Part2/II223.png}
        \caption{Selection 3}
    \end{subfigure}
    \caption{MLS vs MAP}
    \label{fig:II22}
\end{figure}



1:
bestmN = [[-0.25975759], [ 0.99400395]]
bestrms = 33.6875907352
bestAlpha = 10


2:
bestmN = [[ 0.93127615]]
bestrms = 20.8018430001
bestAlpha = 10

3:
bestmN = [[-0.02833114], [ 0.18767638], [ 0.03828809], [-0.57547992], [ 1.35501008]]
bestrms = 14.5218332936
bestAlpha = 10


\subsection{II.2.3}

I dont like crying

\end{document}
